# Hands-on data handling

 - Folder structure
 - meta data
 - Data provenance

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
.blauw {
  background-color: #f3fbfe;
}
</style>

## Introducing Reproducible Research

Reproducible research means that you work in a way that makes it possible for someone else to reach the same results from your data. Within ONTOX, this is an important concept, as reproducibility leads to increased rigour and quality of scientific outputs, and thus to greater trust in our results.Additionally, a reproducible workflow leads to a decrease in headaches over your own project data management, revisions, and using data or analyses from previous projects.  

In general, the most important part of a reproducible workflow around a dataset is giving all meta data on your data (including your hypotheses, study design, specifics on your data collection and all the analysis steps you took, as well as any publications based on this data.) The second most important thing is version control: keeping track of any changes to your dataset, and to any documentation accompanying it. We will start with setting up a folder structure to store this information.

[Click for more information: Symposium report, October 2015. Reproducibility and reliability of biomedical research: improving research  practice](https://acmedsci.ac.uk/viewFile/56314e40aac61.pdf)


## What you need for replication

To replicate a scientific study you would need some information from and information about it. So this is the information that we should be keeping together when working on a project:

- **Context and interpretation** Scientific context! Research questions, related publications..  
    + manuscrips
    + presentations
    + posters
- **Methods** Exactly how the data was gathered 
    + protocols
    + code from experiments
    + ethical committee approvals
- **Data** 
    + the raw data itself
    + meta data (which is data about the data. For instance who gathered it, when, where, licence information, keywords...)
- **Data analysis** All steps in data analysis 
    + Exploratory data analysis of the data 
    + data mangling decisisons, such as fitering and outlier selection
    + processed data
    + Which inductive statistical tests or inference was done and how.

## Gorilla analytics folder structure

I you set up your projects in a way that makes it easy to find all this information, you will save yourself and others a lot of time. To help you start building this we will use the `Guerrilla Analytics` framework, as described by Enda Ridge in [this booklet](https://www.elsevier.com/books/T/A/9780128002186).  

Some key Guerrilla Analytics Principles to remember:

 >1. Space is cheap, confusion is expensive
 >1. Use simple, visual project structures and conventions
 >1. Keep an intact link to data and the dissemination of knowledge (e.g. papers based on the data, emails)
 >1. Version control changes to data and analytics code 
 >1. Consolidate team knowledge (agree on guidelines and stick to it as a team)  

<p style="font-size:14px">[Guerrilla Analytics book by Enda Ridge, ](https://guerrilla-analytics.net/)</p>

### non reproducible folder structure

A quite common way to organise our digital files, is by making categories. For instance, both of your teachers today organised their laptops previously similar to:

```{r, eval=F, include=F}
fs::dir_tree(here::here("wrong_structure"))
```

```
old_laptop/wrong_structure
├── courses
├── ethical approval
├── failed projects
├── images
├── manuscripts
│   ├── me_et al , 2020_v01 - Copy.docx
│   ├── me_et al , 2020_v02.docx
│   └── me_et al , 2020_v03_final_final.docx
├── presentations
├── project A
│   ├── Data files 001
│   ├── Project Documentation
│   ├── Volunteer responses
│   ├── analyses
│   └── revision
│       ├── new analyses
│       ├── new figs
│       └── replies to reviewers
└── protocols

```

This makes sense but has at least two major drawbacks:

 1. Deep nesting of files in folders can cause you to lose files, or the link between files. Most importantly, in 10 years time: will you remember on what dataset a specific powerpoint presentation was based?
 2. Things will change. There will be new categories and new types of files, you had not thought of before. Where will these go?


### reproducible folder structure

So when managing files in projects adhere to the following guidelines

 1. **Create a separate folder for each analytics project** . Keep the unit of a project small. 
 1. **Use the same folder structure in every project** Better even: agree on a folder structure as group.
 1. **Do not deeply nest folders** (max 2-3 levels, so no *project/documentation/applications/metc/first_communication/input/docs/moredocs/mail.txt*)
 1. **Keep information about the data, close to the data**. Save a README.txt in the data folder with for example descriptions about variables and meta data.
 1. **Store each dataset in its own sub-folder.** example below.
 1. **Do not change file names or move them.** Don't change a file name of a file you recieve or download from the internet. If you absolutely have to, record your changes in the README.txt file.
 1. **Do not manually edit data source files.** if you can prevent it. You can (_almost_) always solve this using code. If you absolutely have to, record your changes in the README.txt file.
 1. **In code, use relative paths** (so not mylaptop/users/alyanne/studyA/data/etc) as absolute paths will change over time.

So for instance, your folder structure could look like this:

```{r, eval=F, include=F}
fs::dir_tree(here::here("correct_structure"))
```

```
new_laptop/correct_example_structure
├── courses
├── project A
│   ├── analyses
│   │   ├── code
│   │   └── data
│   ├── data_raw
│   │   └── Volunteer responses
│   ├── documentation
│   ├── ethical approval
│   ├── images
│   ├── manuscript
│   │   ├── me_et al , 2020.docx
│   │   ├── version 1
│   │   └── version 2
│   ├── methods
│   │   └── protocols
│   └── presentations
├── project B
└── project C
```


### exercise: cleaning your own folder structure {.blauw}

15 minutes

Look at your research projects. Which example folder structure does it resemble the most?

 1. What needs to be done to clean the folder structure? 
 1. Try to fix a few things right now.
 1. Book a timeslot (e.g. 1 morning) in your calendar to fix the folder structure of your current projects.



## meta data


## File names

**Do you recognize this?**


```{r, out.width = "300px", echo=FALSE}
knitr::include_graphics(
  file.path(
    "images/final.jpg"
  )
)

```

The use of version control abolishes the need for inventing a file name every time you save it. We will briefly discuss github in the second half of this workshop. With the use of proper version control you only have to think about naming a file just once with a good name. But what entitles a 'good' file name?

A good file name is:

 1. Unique in a folder (prevent duplicated names)
 1. Is short, but descriptive (if you need it to be longer to be descriptive enough, choose that)
 1. Does not contain any special characters* except for `_` and a `.` before the extension. Having multiple dots (`.`) in a file name can be confusing but sometimes is required. For example for an archive we sometimes see `<file_name>.tar.gz`
 1. The typeface of a file name is ideally set in lowercase only. If you want to deviate from this use `UpperFirst` camelcase instead.
 1. The most important thing about naming files is to be consistent. This is also the hardest part!
 1. If you receive a file from somebody else: **Never change the file name, even if it does not meet the above requirements**. Changing  a file name causes a breakage between the file and the source it came from. If you change a file name you recieved from a person or downloaded from the internet, the person who send the file will not know about the new name.

*The special characters you should avoid in a file name:
```
! @ # $ % ^ & * ( ) + - = : " ' ; ? > < ~ / ? { } [ ] \ | ` , 
```

Special characters are reserved for other purposes and can cause problems when a back-up of the files is made or when files need to be loaded in analyzing software or when copying files.

**Basically, what was stated about file names, also applied to naming variables in a dataset.**




## Meta data

### exercise: starting to add meta data files {.blauw}

15 minutes 

Add meta data files to your current project

## Tidy data

Tidy data is a way of organising your data in a neat and structured way. If you make your data tidy, it is ensured that it is machine readable. This is important in ONTOX, as we want to use the data to build an AI platform.

So what is tidy data:

1. Each variable must have its own column.  
1. Each observation must have its own row. 
1. Each value must have its own cell.

*Table1: Example of tidy data*

| country 	| year 	| population 	| birth rate 	|
|:---------	|:-----	|:----------	|:-----------	|
| mali    	| 2001 	| 10.000.000 	| 6.88       	|
| mali    	| 2010 	| 15.000.000 	| 6.06       	|
| sweden  	| 2001 	| 9.000.000  	| 1.57       	|
| sweden  	| 2010 	| 10.000.000 	| 1.85       	|


Each variable must have its own column.  

```{r , echo=FALSE, message=FALSE, out.width = "60%"}
knitr::include_graphics("images/03_4_tidy1.jpg")
```

Each observation must have its own row. 

```{r , echo=FALSE, message=FALSE, out.width = "60%"}
knitr::include_graphics("images/03_5_tidy2.jpg")
```

Each value must have its own cell.

```{r , echo=FALSE, message=FALSE, out.width = "60%"}
knitr::include_graphics("images/03_6_tidy3.jpg")
```

So the following table would by untidy, as there are multiple observations per row:

*Table2: Example of untidy data*

| participant  	| sample1 	| sample2 	| sample3 	| sample4 	|
|:----------	|:------	|:--------	|:-------------	|:-----	|
| Pietje   	| 7.5  	| 6      	| 8.2         	| 8   	|
| Marietje 	| 8    	| 7.9    	| 5           	| 9   	|


This would be the tidy version:

*Table3: Example of untidy data made tidy*

| student  	| sample      	| stairs_walked 	|
|:----------	|:-------------	|:-------	|
| Pietje   	| 1        	| 7.5   	|
| Pietje   	| 2      	| 6     	|
| Pietje   	| 3 	| 8.2   	|
| Pietje   	| 4         	| 8     	|
| Marietje 	| 1        	| 8     	|
| Marietje 	| 2      	| 7.9   	|
| Marietje 	| 3 	| 5     	|
| Marietje 	| 4         	| 9     	|

We will discuss why tidy data is needed in a central discussion next.



## Data provenance

## The _GUI problem_

Most people would agree that this seems to be a good practice. However, many people use GUI based software (graphical user interface). Citing Wikipedia, this is a *"user interface that allows users to interact with electronic devices through graphical icons and audio indicator such as primary notation, instead of text-based user interfaces, typed command labels or text navigation."* So basically, any program that you use by clicking around in menu's, or using voice control etc, for data science this could for instance be Excel.

However, how would you 'describe' the steps of an analysis or creation of a graph when you use GUI based software? 

```{r messysteps, dpi = 80, echo=F}
knitr::include_graphics(
  file.path(
    "images/messy_steps.jpg"
  )
)
```

<p style="font-size:14px">**The file "./Rmd/steps_to_graph_from_excel_file.html" shows you how to do this using the programming language R.</p>

Maybe ~~videotape~~ tiktok-record your process and add .mp4 to your paper? Type out every step in a Word document? Both options seem rather silly. This is actually really hard!

This is why so many researchers are using code to analyse their data. If you do, just make sure you keep the code with the data. If you don't, make sure you precisely keep track of every step you take, in a separate text document.


